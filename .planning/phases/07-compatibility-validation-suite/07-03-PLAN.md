---
phase: 07-compatibility-validation-suite
plan: 03
type: execute
wave: 2
depends_on: [07-01]
files_modified:
  - Cargo.toml
  - benches/template_performance.rs
autonomous: true
requirements: [VAL-03]

must_haves:
  truths:
    - "Developer can run cargo bench to measure template performance"
    - "Developer can detect performance regressions via statistical analysis"
    - "Developer can benchmark individual templates independently"
  artifacts:
    - path: "benches/template_performance.rs"
      provides: "Criterion benchmark suite for templates"
      min_lines: 50
      contains: "criterion_group"
    - path: "Cargo.toml"
      provides: "criterion dependency"
      contains: "criterion"
  key_links:
    - from: "benches/template_performance.rs"
      to: "templates/"
      via: "include_str for embedded templates"
      pattern: "include_str.*templates/"
    - from: "benches/template_performance.rs"
      to: "tests/fixtures/"
      via: "include_str for benchmark inputs"
      pattern: "include_str.*fixtures/"
---

<objective>
Add performance benchmarking suite to detect template parsing regressions.

Purpose: Establish baseline performance metrics and enable statistical detection of performance degradation across code changes.

Output: Criterion-based benchmark suite measuring parsing performance for all embedded templates with statistical rigor.
</objective>

<execution_context>
@/Users/simonknight/.claude/get-shit-done/workflows/execute-plan.md
@/Users/simonknight/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/07-compatibility-validation-suite/07-CONTEXT.md
@.planning/phases/07-compatibility-validation-suite/07-RESEARCH.md
@.planning/phases/07-compatibility-validation-suite/07-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Add criterion dependency and create benchmark file</name>
  <files>
    Cargo.toml
    benches/template_performance.rs
  </files>
  <action>
Add criterion to dev-dependencies in Cargo.toml:

```toml
[dev-dependencies]
criterion = "0.5"

[[bench]]
name = "template_performance"
harness = false
```

Create benches/template_performance.rs with basic structure per Pattern 3 from RESEARCH.md.

Initial structure:
```rust
use criterion::{black_box, criterion_group, criterion_main, Criterion};
use cliscrape::{FsmParser, TemplateFormat};

fn benchmark_placeholder(c: &mut Criterion) {
    c.bench_function("placeholder", |b| {
        b.iter(|| {
            black_box(42)
        })
    });
}

criterion_group!(benches, benchmark_placeholder);
criterion_main!(benches);
```

Verify compilation:
```bash
cargo bench --no-run
```

This establishes the benchmark infrastructure before adding actual template benchmarks in the next task.
  </action>
  <verify>
cargo bench --no-run
ls benches/
  </verify>
  <done>
criterion 0.5 in dev-dependencies. benches/template_performance.rs exists and compiles. cargo bench --no-run succeeds.
  </done>
</task>

<task type="auto">
  <name>Implement per-template benchmarks with criterion</name>
  <files>
    benches/template_performance.rs
  </files>
  <action>
Implement benchmarks for all 5 embedded templates following Pattern 3 from RESEARCH.md.

For each template, create a benchmark function:

```rust
fn benchmark_cisco_ios_show_version(c: &mut Criterion) {
    let template_content = include_str!("../templates/cisco_ios_show_version.yaml");
    let input = include_str!("../tests/fixtures/cisco/ios_show_version/ios_15_standard.txt");
    let parser = FsmParser::from_str(template_content, TemplateFormat::Yaml).unwrap();

    c.bench_function("cisco_ios_show_version", |b| {
        b.iter(|| parser.parse(black_box(input)))
    });
}
```

Create benchmarks for:
1. cisco_ios_show_version
2. cisco_ios_show_interfaces
3. cisco_nxos_show_version
4. juniper_junos_show_version
5. arista_eos_show_version

Update criterion_group to include all benchmarks:
```rust
criterion_group!(
    benches,
    benchmark_cisco_ios_show_version,
    benchmark_cisco_ios_show_interfaces,
    benchmark_cisco_nxos_show_version,
    benchmark_juniper_junos_show_version,
    benchmark_arista_eos_show_version
);
```

Use positive test fixtures (from 07-01) as benchmark inputs - they represent realistic parsing scenarios.

Use black_box() around input to prevent compiler from optimizing away the parse operation.

Do NOT set performance regression thresholds yet - this establishes baseline only (per RESEARCH.md Open Question 2: "Start with reporting only, no CI failure").
  </action>
  <verify>
cargo bench
  </verify>
  <done>
template_performance.rs contains 5 benchmark functions. cargo bench runs successfully and reports performance metrics for each template. Criterion generates statistical analysis with confidence intervals.
  </done>
</task>

</tasks>

<verification>
Run benchmark suite:
```bash
cargo bench
```

Verify benchmark output includes:
- Time measurements for each template
- Confidence intervals
- Change detection (after first run)

Check criterion generated reports:
```bash
ls target/criterion/
```

Verify all 5 templates benchmarked:
```bash
cargo bench 2>&1 | grep -E "(cisco_ios|cisco_nxos|juniper|arista)"
```
</verification>

<success_criteria>
- criterion 0.5.x in dev-dependencies
- benches/template_performance.rs compiles
- 5 benchmark functions (one per embedded template)
- cargo bench runs successfully
- Benchmark output shows statistical analysis with confidence intervals
- Criterion reports generated in target/criterion/
- No performance regression failures (baseline establishment only)
</success_criteria>

<output>
After completion, create `.planning/phases/07-compatibility-validation-suite/07-03-SUMMARY.md`
</output>
